{
 "metadata": {
  "name": "",
  "signature": "sha256:d58afc2be7e11c09c90a9943c4e07c62856c6e5ac6b6a72ce862287baf02a8df"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle as pickle\n",
      "import re\n",
      "import random\n",
      "from os import path\n",
      "\n",
      "import nltk\n",
      "\n",
      "import testing_util as util\n",
      "import term_scoring\n",
      "from testing_util import sample_sets, final_sets\n",
      "import tfidf_class"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stopwords = nltk.corpus.stopwords.words('english')\n",
      "stemmer = nltk.PorterStemmer()\n",
      "\n",
      "def get_terms(t):\n",
      "    \"\"\"\n",
      "    This is used to get the relevant items out of text. \n",
      "    \"\"\"\n",
      "    tokens = nltk.word_tokenize(t)\n",
      "    return [stemmer.stem(w.lower()) for w in tokens if len(w)>3 and (w not in stopwords)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cats= tfidf_class.get_cat_idfs(sample_sets, get_terms, [str(i) for i in range(1,8)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for t in cats[\"3\"][0:50]:\n",
      "    print \"{0:<16}{1:<10.3%}{2:<10.3%}{3:<10.3%}\".format(*t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "movi            9.039%    9.606%    0.567%    \n",
        "song            7.954%    8.128%    0.175%    \n",
        "music           5.229%    5.665%    0.436%    \n",
        "favorit         5.098%    5.665%    0.567%    \n",
        "show            4.880%    5.665%    0.785%    \n",
        "first           2.991%    5.172%    2.182%    \n",
        "rock            2.868%    2.956%    0.087%    \n",
        "name            2.339%    4.433%    2.094%    \n",
        "magazin         2.173%    2.217%    0.044%    \n",
        "episod          2.173%    2.217%    0.044%    \n",
        "watch           2.055%    2.709%    0.654%    \n",
        "ever            2.008%    3.448%    1.440%    \n",
        "listen          1.999%    2.217%    0.218%    \n",
        "lyric           1.970%    1.970%    0.000%    \n",
        "seri            1.840%    1.970%    0.131%    \n",
        "like            1.814%    8.621%    6.806%    \n",
        "band            1.796%    1.970%    0.175%    \n",
        "kill            1.796%    1.970%    0.175%    \n",
        "heard           1.765%    2.463%    0.698%    \n",
        "film            1.724%    1.724%    0.000%    \n",
        "singer          1.681%    1.724%    0.044%    \n",
        "riddl           1.681%    1.724%    0.044%    \n",
        "celebr          1.681%    1.724%    0.044%    \n",
        "artist          1.681%    1.724%    0.044%    \n",
        "album           1.434%    1.478%    0.044%    \n",
        "cartoon         1.434%    1.478%    0.044%    \n",
        "sing            1.347%    1.478%    0.131%    \n",
        "rememb          1.331%    1.724%    0.393%    \n",
        "download        1.329%    2.463%    1.134%    \n",
        "play            1.301%    2.217%    0.916%    \n",
        "free            1.298%    2.956%    1.658%    \n",
        "sinc            1.288%    1.724%    0.436%    \n",
        "thought         1.288%    1.724%    0.436%    \n",
        "could           1.282%    3.202%    1.920%    \n",
        "star            1.260%    1.478%    0.218%    \n",
        "figur           1.244%    1.724%    0.480%    \n",
        "pari            1.232%    1.232%    0.000%    \n",
        "hilton          1.232%    1.232%    0.000%    \n",
        "think           1.218%    4.926%    3.709%    \n",
        "radio           1.188%    1.232%    0.044%    \n",
        "actor           1.144%    1.232%    0.087%    \n",
        "publish         1.144%    1.232%    0.087%    \n",
        "group           1.129%    1.478%    0.349%    \n",
        "street          1.101%    1.232%    0.131%    \n",
        "green           1.057%    1.232%    0.175%    \n",
        "famou           1.057%    1.232%    0.175%    \n",
        "heart           1.013%    1.232%    0.218%    \n",
        "howard          0.985%    0.985%    0.000%    \n",
        "dvd             0.985%    0.985%    0.000%    \n",
        "simpson         0.985%    0.985%    0.000%    \n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for c in [str(i) for i in range(1,8)]:\n",
      "    tokens = [w for s in sample_sets for w in nltk.tokenize.word_tokenize(s[1]) if s[0] == c]\n",
      "    tokens = [t for t in tokens if (not re.findall(\"[']|what|which|where|have|why|they\", t, re.I)) and len(t)>3]\n",
      "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
      "    finder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
      "    finder.apply_freq_filter(2)\n",
      "    print c\n",
      "    print finder.nbest(bigram_measures.pmi, 20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n",
        "[('april', '1972'), ('beef', 'jerky'), ('boise', 'idaho'), ('cliffnotes.com', 'helpful'), ('datsun', 'parts'), ('design', 'analytical'), ('former', 'employer'), ('helping', 'design'), ('michael', 'jackson'), ('midlife', 'crisis'), ('mutual', 'funds'), ('neighbor', 'rents'), ('social', 'sercurity'), ('timberlin', 'boots'), ('collection', 'agency'), ('hurricane', 'katrina'), ('message', 'boards'), ('asset', 'classes'), ('britain', 'pounds'), ('dispute.', 'legally')]\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('blinks', 'quickly'), ('brokerage', 'firm'), ('clients', 'attached'), ('dvd+r', 'dvd-r'), ('factory', 'condition'), ('mcse', 'linux+'), ('netgear', 'hr314'), ('paid', 'inclusion'), ('paul', 'wall'), ('separate', 'subnet'), ('slideshow', 'creation'), ('boot', 'linux/windows'), ('contents', 'textarea'), ('erase', 'subjects'), ('fedora', 'core'), ('on-line', 'brokerage'), ('quoted', 'text'), ('rear', 'audio'), ('resume', 'admission'), ('sockets', 'java.')]\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('1959', 'gibson'), ('c-tech', 'astronomy'), ('calvin', 'hobbes'), ('chronicles', 'narnia'), ('jackson', 'films'), ('laura', 'palmer'), ('lock', 'hair'), ('monty', 'python'), ('paula', 'abdul'), ('peter', 'jackson'), ('sarah', 'mcglocklin'), ('soap', 'operas'), ('bruce', 'springsteen'), ('cheapest', 'source'), ('html', 'address'), ('john', 'lennon'), ('running', 'backstreets'), ('shot', 'burns'), ('snow', 'castle'), ('source', 'ordering')]\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('answered', 'naked'), ('engagement', 'ring'), ('exexex', 'kept'), ('jesse', 'mccartney'), ('kept', 'breaking'), ('prefer', 'large'), ('during', 'softball'), ('split', 'twice'), ('winter', 'dance'), ('notice', 'answered'), ('size', 'matter'), ('well', 'soon'), ('soul', 'mate'), ('forgive', 'forget'), ('keeps', 'telling'), ('meet', 'internet'), ('long', 'distance'), ('pretty', 'grrrs'), ('slow', 'down'), ('finding', 'nice')]\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('1410', '1533.'), ('1533.', 'city'), ('ad600-1432.', 'modern-day'), ('andean', 'incan'), ('daylight', 'savings'), ('developing', 'bird'), ('duck', 'walk'), ('encompasses', 'territory'), ('ever', 'measured'), ('fire', 'trucks'), ('gold', 'chains'), ('lanes', 'across'), ('modern-day', 'nation'), ('nation', 'encompasses'), ('rhetorical', 'device'), ('seeds', 'travel'), ('sophisticated', 'andean'), ('temperature', 'ever'), ('animals', 'found'), ('change', 'lanes')]\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('called', 'core'), ('hepatitis', 'infection'), ('homemade', 'salve'), ('making', 'homemade'), ('names', 'strengthen'), ('cigarette', 'smoke'), ('fall', 'asleep'), ('gained', 'pounds'), ('indiana', 'ohio'), ('over-the-counter', 'strength'), ('strength', 'tablets'), ('supplement', 'names'), ('these', 'rashes'), ('difference', 'between'), ('herpes', 'simplex'), ('anxiety', 'problems'), ('side', 'effects'), ('treatment', 'options'), ('make', 'sick'), ('lose', 'weight')]\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('dodo', 'birds'), ('eating', 'four'), ('idiopathic', 'senile'), ('k-means', 'clustering'), ('magic', 'cube'), ('night', 'vision'), ('peripheral', 'neuropathy'), ('senile', 'peripheral'), ('stations', 'ethanol'), ('wyoming', 'windy'), ('bond', 'breaks'), ('capital', 'city'), ('central', 'observations'), ('cluster', 'centroids'), ('covalent', 'bond'), ('fibbonacci', 'sequence'), ('first', 'chicken'), ('infinite', 'conducting'), ('penguins', 'survive'), ('blueish', 'color.')]\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}