{
 "metadata": {
  "name": "",
  "signature": "sha256:7f202a0f6f8914e37e6a3db1320156e1091fd233f2e6d04241c36ba0961da43e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import random\n",
      "import string\n",
      "from os import path\n",
      "\n",
      "import nltk\n",
      "\n",
      "import testing_util as util\n",
      "import term_scoring\n",
      "from testing_util import sample_sets, final_sets\n",
      "\n",
      "import tfidf_class"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def feature_num_tokens(text):\n",
      "    return {\"num_tokens\": len(nltk.word_tokenize(text))/10}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def feature_avg_len_word(text):\n",
      "    tokens = nltk.word_tokenize(text)\n",
      "    return {\"word_len\": sum([len(w) for w in tokens])/len(tokens)}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stopwords = nltk.corpus.stopwords.words('english')\n",
      "stemmer = nltk.PorterStemmer()\n",
      "\n",
      "def get_terms(t):\n",
      "    \"\"\"\n",
      "    This is used to get the relevant items out of text. \n",
      "    \"\"\"\n",
      "    tokens = nltk.word_tokenize(t)\n",
      "    return [stemmer.stem(w.lower()) for w in tokens if len(w)>3 and (w not in stopwords)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_bigrams(t):\n",
      "    \"\"\"\n",
      "    This is used to get the relevant items out of text. \n",
      "    \"\"\"\n",
      "    tokens = nltk.word_tokenize(t)\n",
      "    tokens = [stemmer.stem(w.lower()) for w in tokens if len(w)>3 and (w not in stopwords)]\n",
      "    \n",
      "    return nltk.bigrams(tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def verb_tagging(sentence):\n",
      "    \n",
      "    # spliting Corpus into sentences\n",
      "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "    sents = sent_tokenizer.tokenize(sentence.strip())\n",
      "    \n",
      "    gerunds = []\n",
      "    pattern = r'''(?x)\n",
      "    ([A-Z]\\.\\s[A-Z]\\.\\s\\w+)\n",
      "    |([A-Z]\\.)\\s([A-Z]\\.)\n",
      "    |([A-Z]\\.)+\n",
      "    |\\w+([-']\\w+)*\n",
      "    |\\$?\\d+(\\.\\d+)?%?\n",
      "    |[.,?;]+\n",
      "    '''\n",
      "    \n",
      "    for sent in sents:\n",
      "\n",
      "        words_list = nltk.regexp_tokenize(sent, pattern)\n",
      "        words = [word for word in words_list if word not in string.punctuation] #filter out unwanted words\n",
      "\n",
      "        sentence_tagged = nltk.pos_tag(words) #find position tagging\n",
      "\n",
      "        verbs = [v[0] for v in sentence_tagged if re.match('V.*', v[1])]\n",
      "\n",
      "        gerunds.extend(verbs)\n",
      "    \n",
      "    return gerunds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def category_term_scorer1(sample_list):\n",
      "    \"\"\"\n",
      "    takes a list of tuples in format (category,text) and creates scores for each term\n",
      "    for its relevance to each category\n",
      "    \"\"\"\n",
      "    categories = {}\n",
      "    \n",
      "    for c,s in sample_list:\n",
      "        if c not in categories:\n",
      "            categories[c]=[]\n",
      "        for w in get_terms(s):\n",
      "            categories[c].append(w)\n",
      "    \n",
      "    fd_all = nltk.FreqDist([w for wl in categories.values() for w in wl])\n",
      "    \n",
      "    fd_categories = {c:nltk.FreqDist(v) for c,v in categories.iteritems()}\n",
      "    \n",
      "    term_scores = {}\n",
      "    for term in fd_all.iterkeys():\n",
      "        if fd_all[term] < 5:\n",
      "            continue\n",
      "        d = {}\n",
      "        for c,fd in fd_categories.iteritems():\n",
      "            d[c]= fd[term]/(1+fd_all[term]-fd[term])\n",
      "        term_scores[term]=d\n",
      "    \n",
      "    return term_scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def category_term_scorer2(sample_list):\n",
      "    \"\"\"\n",
      "    takes a list of tuples in format (category,text) and creates scores for each term\n",
      "    for its relevance to each category\n",
      "    \"\"\"\n",
      "    categories = {}\n",
      "    \n",
      "    for c,s in sample_list:\n",
      "        if c not in categories:\n",
      "            categories[c]=[]\n",
      "        for w in get_bigrams(s):\n",
      "            categories[c].append(w)\n",
      "    \n",
      "    fd_all = nltk.FreqDist([w for wl in categories.values() for w in wl])\n",
      "    \n",
      "    fd_categories = {c:nltk.FreqDist(v) for c,v in categories.iteritems()}\n",
      "    \n",
      "    term_scores = {}\n",
      "    for term in fd_all.iterkeys():\n",
      "        if fd_all[term] < 5:\n",
      "            continue\n",
      "        d = {}\n",
      "        for c,fd in fd_categories.iteritems():\n",
      "            d[c]= fd[term]/(1+fd_all[term]-fd[term])\n",
      "        term_scores[term]=d\n",
      "    \n",
      "    return term_scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def category_term_scorer3(sample_list):\n",
      "    \"\"\"\n",
      "    takes a list of tuples in format (category,text) and creates scores for each term\n",
      "    for its relevance to each category\n",
      "    \"\"\"\n",
      "    categories = {}\n",
      "    \n",
      "    for c,s in sample_list:\n",
      "        if c not in categories:\n",
      "            categories[c]=[]\n",
      "        for w in verb_tagging(s):\n",
      "            categories[c].append(w)\n",
      "    \n",
      "    fd_all = nltk.FreqDist([w for wl in categories.values() for w in wl])\n",
      "    \n",
      "    fd_categories = {c:nltk.FreqDist(v) for c,v in categories.iteritems()}\n",
      "    \n",
      "    term_scores = {}\n",
      "    for term in fd_all.iterkeys():\n",
      "        if fd_all[term] < 5:\n",
      "            continue\n",
      "        d = {}\n",
      "        for c,fd in fd_categories.iteritems():\n",
      "            d[c]= fd[term]/(1+fd_all[term]-fd[term])\n",
      "        term_scores[term]=d\n",
      "    \n",
      "    return term_scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "termscore1 = term_scoring.TermScoreClassiffier(scorer=category_term_scorer1, key=\"termscore\", tokenizer=get_terms)\n",
      "termscore2 = term_scoring.TermScoreClassiffier(scorer=category_term_scorer2, key=\"bigramscore\", tokenizer=get_bigrams)\n",
      "termscore2 = term_scoring.TermScoreClassiffier(scorer=category_term_scorer2, key=\"verbscore\", tokenizer=verb_tagging)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf_checker = tfidf_class.TfidfRater(get_terms, nterms=50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "extractor = util.FeatureExtractor()\n",
      "extractor.add_extractor(feature_num_tokens)\n",
      "extractor.add_extractor(feature_avg_len_word)\n",
      "extractor.add_extractor(termscore1, trained=True)\n",
      "extractor.add_extractor(termscore2, trained=True)\n",
      "extractor.add_extractor(tfidf_checker, trained=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "extractor.test(sample_sets, folds=10, confusion=True, classifier=nltk.classify.NaiveBayesClassifier)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "**************************\n",
        "Run 0\n",
        "**************************\n",
        "test 0 - 58.364%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  |      1      2      3      4      5      7      6 |\n",
        "--+--------------------------------------------------+\n",
        "1 | <17.5%>  3.0%      .   1.5%   0.7%   0.4%   0.4% |\n",
        "2 |   3.0% <16.0%>  0.4%      .   1.1%   0.4%      . |\n",
        "3 |   8.6%   0.4%  <5.6%>  0.7%      .   1.1%      . |\n",
        "4 |   3.0%      .   1.1%  <8.2%>     .      .   0.4% |\n",
        "5 |   3.7%   0.7%      .      .  <5.6%>  1.1%   0.7% |\n",
        "7 |   5.9%      .      .   0.7%      .  <4.1%>  0.4% |\n",
        "6 |   2.2%      .      .      .      .      .  <1.5%>|\n",
        "--+--------------------------------------------------+\n",
        "(row = reference; col = test)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "test 1 - 49.071%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  |      1      2      6      3      7      4      5 |\n",
        "--+--------------------------------------------------+\n",
        "1 | <16.7%>  4.5%   0.7%   1.1%   1.5%   2.6%   1.1% |\n",
        "2 |   4.1%  <9.3%>     .      .   0.4%      .      . |\n",
        "6 |   6.7%   0.7%  <5.2%>     .   0.4%   0.4%      . |\n",
        "3 |   6.7%   0.7%      .  <4.8%>     .   0.7%      . |\n",
        "7 |   6.3%   1.5%   0.7%   0.4%  <2.2%>     .   1.5% |\n",
        "4 |   2.2%      .      .      .   0.4%  <7.1%>     . |\n",
        "5 |   3.0%   0.7%      .   0.4%   0.4%   1.1%  <3.7%>|\n",
        "--+--------------------------------------------------+\n",
        "(row = reference; col = test)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-13-d8b82cf49303>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mextractor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_sets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/home/paul/Programming/MIMS/nlp2014_kaggle/testing_util.pyc\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self, samples, folds, classifier, num_tests, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m             \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Run {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[1;32mprint\u001b[0m \u001b[1;34m\"**************************\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m             \u001b[0mfold_test_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/paul/Programming/MIMS/nlp2014_kaggle/testing_util.pyc\u001b[0m in \u001b[0;36mfold_test_extractor\u001b[1;34m(feature_extractor, samples, folds, classifier, confusion, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[0mfeature_extractor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_sets\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m  \u001b[0mfolds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/paul/Programming/MIMS/nlp2014_kaggle/testing_util.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrained_extractors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                 \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/paul/Programming/MIMS/nlp2014_kaggle/term_scoring.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You must train the scorer\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-6-ff9334a9f47e>\u001b[0m in \u001b[0;36mverb_tagging\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_list\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#filter out unwanted words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0msentence_tagged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#find position tagging\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mverbs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence_tagged\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'V.*'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/paul/anaconda/envs/nlp/lib/python2.7/site-packages/nltk/tag/__init__.pyc\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \"\"\"\n\u001b[0;32m     99\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_POS_TAGGER\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbatch_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/paul/anaconda/envs/nlp/lib/python2.7/site-packages/nltk/tag/sequential.pyc\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/paul/anaconda/envs/nlp/lib/python2.7/site-packages/nltk/tag/sequential.pyc\u001b[0m in \u001b[0;36mtag_one\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtagger\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_taggers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/paul/anaconda/envs/nlp/lib/python2.7/site-packages/nltk/tag/sequential.pyc\u001b[0m in \u001b[0;36mchoose_tag\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;31m# higher than that cutoff first; otherwise, return None.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cutoff_prob\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m             \u001b[0mpdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/paul/anaconda/envs/nlp/lib/python2.7/site-packages/nltk/classify/maxent.pyc\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/paul/anaconda/envs/nlp/lib/python2.7/site-packages/nltk/classify/maxent.pyc\u001b[0m in \u001b[0;36mprob_classify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mprob_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m             \u001b[0mfeature_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_logarithmic\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/paul/anaconda/envs/nlp/lib/python2.7/site-packages/nltk/classify/maxent.pyc\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, featureset, label)\u001b[0m\n\u001b[0;32m    546\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Inherit docs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m         \u001b[1;31m# Convert input-features to joint-features:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl = extractor.get_classifier(sample_sets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl.show_most_informative_features(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Most Informative Features\n",
        "               termscore = '4'                 4 : 2      =    224.9 : 1.0\n",
        "               termscore = '6'                 6 : 2      =    143.0 : 1.0\n",
        "               termscore = '5'                 5 : 2      =     69.6 : 1.0\n",
        "               termscore = '7'                 7 : 2      =     65.3 : 1.0\n",
        "               termscore = '3'                 3 : 6      =     52.4 : 1.0\n",
        "             has(window) = 1                   2 : 1      =     42.3 : 1.0\n",
        "               has(pain) = 1                   6 : 1      =     31.9 : 1.0\n",
        "             has(comput) = 1                   2 : 4      =     29.8 : 1.0\n",
        "              has(music) = 1                   3 : 1      =     29.6 : 1.0\n",
        "               termscore = '2'                 2 : 4      =     27.3 : 1.0\n",
        "               has(song) = 1                   3 : 1      =     25.3 : 1.0\n",
        "               has(caus) = 1                   6 : 1      =     25.3 : 1.0\n",
        "               has(girl) = 1                   4 : 7      =     24.9 : 1.0\n",
        "       has(relationship) = 1                   4 : 1      =     24.3 : 1.0\n",
        "             has(togeth) = 1                   4 : 1      =     24.3 : 1.0\n",
        "               has(movi) = 1                   3 : 4      =     22.5 : 1.0\n",
        "              has(studi) = 1                   5 : 1      =     22.0 : 1.0\n",
        "          has(boyfriend) = 1                   4 : 3      =     21.4 : 1.0\n",
        "         has(girlfriend) = 1                   4 : 1      =     21.4 : 1.0\n",
        "              has(medic) = 1                   6 : 1      =     20.9 : 1.0\n",
        "             has(colleg) = 1                   5 : 2      =     20.4 : 1.0\n",
        "            has(marriag) = 1                   4 : 1      =     19.9 : 1.0\n",
        "             has(friend) = 1                   4 : 5      =     19.5 : 1.0\n",
        "           has(internet) = 1                   2 : 3      =     18.8 : 1.0\n",
        "               has(bodi) = 1                   6 : 1      =     18.7 : 1.0\n",
        "             has(weight) = 1                   6 : 1      =     18.7 : 1.0\n",
        "              has(cheat) = 1                   4 : 1      =     18.4 : 1.0\n",
        "              has(yahoo) = 1                   2 : 5      =     18.1 : 1.0\n",
        "               has(feel) = 1                   4 : 2      =     17.9 : 1.0\n",
        "                has(use) = 1                   2 : 4      =     17.6 : 1.0\n",
        "             has(school) = 1                   5 : 1      =     17.5 : 1.0\n",
        "            has(softwar) = 1                   2 : 1      =     16.6 : 1.0\n",
        "              has(blood) = 1                   6 : 1      =     16.5 : 1.0\n",
        "              has(smoke) = 1                   6 : 1      =     16.5 : 1.0\n",
        "               has(love) = 1                   4 : 6      =     16.2 : 1.0\n",
        "               has(rock) = 1                   3 : 1      =     15.8 : 1.0\n",
        "            has(program) = 1                   2 : 3      =     15.2 : 1.0\n",
        "             has(realli) = 1                   4 : 5      =     14.3 : 1.0\n",
        "              has(woman) = 1                   4 : 1      =     13.7 : 1.0\n",
        "              has(marri) = 1                   4 : 1      =     13.7 : 1.0\n",
        "               has(keep) = 1                   4 : 3      =     13.6 : 1.0\n",
        "            has(univers) = 1                   7 : 1      =     13.5 : 1.0\n",
        "               has(moon) = 1                   7 : 1      =     13.5 : 1.0\n",
        "             has(instal) = 1                   2 : 1      =     13.2 : 1.0\n",
        "              has(fight) = 1                   6 : 1      =     12.5 : 1.0\n",
        "            has(magazin) = 1                   3 : 1      =     12.0 : 1.0\n",
        "             has(episod) = 1                   3 : 1      =     12.0 : 1.0\n",
        "              has(video) = 1                   2 : 1      =     11.8 : 1.0\n",
        "              has(never) = 1                   4 : 2      =     11.8 : 1.0\n",
        "            has(english) = 1                   5 : 2      =     11.8 : 1.0\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "final_features = util.make_feature(extractor, final_sets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "submission = util.make_submission(cl, final_features, writeto=\"submission28.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    }
   ],
   "metadata": {}
  }
 ]
}